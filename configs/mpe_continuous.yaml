# Model loading (optional)
# Set this to continue training from an existing model or for view.py
load_model: null  # Example: "files/Models/mpe_continuous/best_model.pth"

environment:
  name: mpe_simple_spread
  env_kwargs:
    N: 3  # Number of agents
    local_ratio: 0.5  # Balance between local and global rewards
    max_cycles: 25
    continuous_actions: true  # CONTINUOUS ACTION SPACE
    render_mode: null
  apply_wrappers: true

training:
  episodes: 1500  # More episodes for continuous learning
  max_steps_per_episode: 25
  gamma: 0.98  # Slightly lower for continuous control
  learning_rate: 5e-4  # Higher learning rate for continuous actions
  epsilon_clip: 0.1  # Smaller clip range for continuous
  K_epochs: 6  # More epochs for continuous learning
  c_entropy: 0.01  # Lower entropy for more deterministic continuous control
  max_grad_norm: 0.5
  c_value: 0.5
  lam: 0.95
  batch_size: 64  # Larger batch for continuous learning
  min_learning_rate: 1e-6
  similarity_loss_coef: 0.1  # Moderate cooperation for continuous coordination

logging:
  log_interval: 25
  save_interval: 150
  eval_interval: 75
  save_best_model: true

model:
  num_heads: 4
  embedding_dim: 256
  hidden_size: 512
  save_dir: "files/Models"
  model_name: "TAAC_mpe_continuous"

# Continuous Action Space Notes:
# - MPE continuous actions are 2D forces in range [-1, 1]
# - Requires different hyperparameters than discrete spaces
# - Lower epsilon_clip for stable continuous control
# - Higher learning rate for faster convergence
# - More training epochs to learn continuous mappings
# - Lower entropy coefficient for deterministic control 