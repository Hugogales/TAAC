# Model loading (optional)
# Set this to continue training from an existing model or for view.py
load_model: null  # Example: "files/Models/mats_gym/best_model.pth"

environment:
  name: mats_gym
  env_kwargs:
    scenario: "straight"  # Default scenario
    num_agents: 3
    max_episode_steps: 1000
    reward_mech: "global"  # Can be "global", "local", or "mixed"
    render_mode: null
  apply_wrappers: true

training:
  episodes: 2500
  max_steps_per_episode: 1000
  gamma: 0.99
  learning_rate: 2e-4
  epsilon_clip: 0.2
  K_epochs: 8
  c_entropy: 0.02
  max_grad_norm: 0.5
  c_value: 0.5
  lam: 0.95
  batch_size: 96
  min_learning_rate: 1e-6
  similarity_loss_coef: 0.15
  num_parallel: 3  # Number of parallel environments (3 for complex traffic scenarios)

# Model Architecture
model:
  num_heads: 6
  embedding_dim: 384
  hidden_size: 768

# Logging & Evaluation
logging:
  log_interval: 25      # Print stats every N episodes
  save_interval: 250    # Save model every N episodes  
  eval_interval: 100    # Evaluate model every N episodes
  eval_episodes: 5      # Number of episodes for evaluation

# Output Configuration
output:
  experiment_name: "mats_gym_experiment"
  model_save_path: "files/Models/mats_gym/"
  log_save_path: "experiments/mats_gym/"

# Environment-specific notes:
# - MATS Gym focuses on multi-agent coordination in traffic scenarios
# - Scenario can be changed: "straight", "intersection", "roundabout", etc.
# - Reward mechanism affects cooperation requirements
# - Larger network for complex traffic modeling 